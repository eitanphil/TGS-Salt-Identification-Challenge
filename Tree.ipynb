{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "\n",
    "from scipy.signal import hilbert\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "from skimage.morphology import square, dilation, disk\n",
    "from skimage.feature import canny\n",
    "from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,denoise_wavelet, estimate_sigma)\n",
    "from skimage.filters import median\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.color import rgb2gray\n",
    "import cv2\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge, Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "\n",
    "#from Networks import *\n",
    "#from Comp2_func import *\n",
    "#from IOU import *\n",
    "\n",
    "#from datetime import datetime\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def im_load():\n",
    "    img_path= 'train_data/train_images/'\n",
    "    msk_path= 'train_data/train_masks/'\n",
    "    imx_bank= []\n",
    "    imy_bank= []\n",
    "    depths= []\n",
    "    depth_data= pd.read_csv('depths.csv')\n",
    "    for root, dirs, files in os.walk(img_path + '.'):  \n",
    "        for file_name in files:\n",
    "            imx= imread(img_path + file_name)\n",
    "            imy= imread(msk_path + file_name)\n",
    "            depth= depth_data['z'][depth_data['id']==file_name.split('.')[0]]\n",
    "\n",
    "            imx= rgb2gray(imx)\n",
    "            imy= imy / 65535\n",
    "            if imx.max()>0:\n",
    "                \n",
    "                #imx= (imx - np.mean(imx)) / np.std(imx)\n",
    "                size= (63, 63)\n",
    "                imx= cv2.resize(imx, size)\n",
    "                imy= cv2.resize(imy, size)\n",
    "                \n",
    "                imx_bank.append(imx)\n",
    "                imy_bank.append(imy)\n",
    "                depths.append(depth.values * np.ones((63*63, )).astype(int))  \n",
    "\n",
    "    imx_bank= np.array(imx_bank)\n",
    "    imy_bank= np.array(imy_bank)\n",
    "    depths= np.array(depths)\n",
    "    return imx_bank, imy_bank, depths\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "\n",
    "def BP_filter(data, lowcut, highcut, fs, order, axis):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    if axis=='vertical':\n",
    "        data= np.rot90(data)\n",
    "    y = lfilter(b, a, data)\n",
    "    if axis== 'vertical':\n",
    "        y= np.rot90(y, k=1, axes=(1,0))\n",
    "    return y\n",
    "\n",
    "def BP_filter90(data, lowcut, highcut, fs, order):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    #data= np.rot90(data)\n",
    "    y = lfilter(b, a, data)\n",
    "    #y= np.rot90(y, k=1, axes=(1,0))\n",
    "    return y\n",
    "\n",
    "def bp(X, lowcut, highcut, order, axis):\n",
    "    imx_bank= []\n",
    "    for imx in X:\n",
    "        bp= BP_filter(imx, lowcut=lowcut, highcut=highcut, fs=1/0.004, order=order, axis=axis)\n",
    "        imx_bank.append(bp/1.3)\n",
    "    imx_bank= np.array(imx_bank)\n",
    "    return imx_bank\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1 - dice_coef(y_true, y_pred)\n",
    "\n",
    "def binary_dice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) + (1 - dice_coef(y_true, y_pred))\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    \"\"\"\n",
    "    Dice = (2*|X & Y|)/ (|X|+ |Y|)\n",
    "         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n",
    "    ref: https://arxiv.org/pdf/1606.04797v1.pdf\n",
    "    \"\"\"\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\n",
    "\n",
    "def dice_coef1(y_true, y_pred, smooth=1):\n",
    "    \"\"\"\n",
    "    Dice = (2*|X & Y|)/ (|X|+ |Y|)\n",
    "         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n",
    "    ref: https://arxiv.org/pdf/1606.04797v1.pdf\n",
    "    \"\"\"\n",
    "    intersection = np.sum(np.abs(y_true * y_pred), axis=-1)\n",
    "    return (2. * intersection + smooth) / (np.sum(np.square(y_true),-1) + np.sum(np.square(y_pred),-1) + smooth)\n",
    "\n",
    "\n",
    "# Show images\n",
    "def show_im(X, y, imp):\n",
    "    #imy= np.squeeze(y)\n",
    "    #imx_show= np.squeeze(X)\n",
    "    #imp= np.squeeze(imp, axis=[0, -1])\n",
    "\n",
    "    fig, (ax1, ax2, ax3)= plt.subplots(1, 3)\n",
    "    ax1.imshow(X)\n",
    "    ax2.imshow(y)\n",
    "    ax3.imshow(imp)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Loading images')\n",
    "X, y, d= im_load()\n",
    "X= X.astype(np.float32)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-liniar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X2= (X**2).astype(np.float32)\n",
    "Xlog= np.log1p(X).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency BandPass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vertical axis\n",
    "variations= []\n",
    "low= [0.1, 2, 5, 10, 20] \n",
    "high= [7, 10, 20, 30, 40]\n",
    "order= [2, 3, 5]\n",
    "variations_prod= list(product(*[low, high, order]))\n",
    "[variations.append(x) for x in variations_prod if x[1]>x[0]]\n",
    "\n",
    "XBV= np.zeros((X.shape[0] * X.shape[1] * X.shape[2], len(variations)))\n",
    "for i, variation in enumerate(variations): \n",
    "    XBV[:, i]= bp(X, lowcut=variation[0], highcut=variation[1], order=variation[2], axis='vertical').flatten() #reshape((X.shape[0], X.shape[1] * X.shape[2] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Horizontal axis\n",
    "variations= []\n",
    "low= [0.1, 2, 5, 10, 20] \n",
    "high= [7, 10, 20, 30, 40]\n",
    "order= [2, 3, 5]\n",
    "variations_prod= list(product(*[low, high, order]))\n",
    "[variations.append(x) for x in variations_prod if x[1]>x[0]]\n",
    "\n",
    "XBH= np.zeros((X.shape[0] * X.shape[1] * X.shape[2], len(variations)))\n",
    "for i, variation in enumerate(variations): \n",
    "    XBH[:, i]= bp(X, lowcut=variation[0], highcut=variation[1], order=variation[2], axis='horizontal').flatten()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analitic Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs= 1/0.004\n",
    "analytic_signal = hilbert(X)\n",
    "\n",
    "amplitude_envelope = np.abs(analytic_signal)\n",
    "envelope_derv1= np.insert((np.diff(amplitude_envelope)), 0, 0, axis=-1)\n",
    "envelope_derv2= np.insert((np.diff(envelope_derv1)),0, 0, axis=-1)\n",
    "instantaneous_phase = np.unwrap(np.angle(analytic_signal))\n",
    "instantaneous_frequency = np.insert((np.diff(instantaneous_phase) / (2.0*np.pi) * fs),0, 0, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X_f= np.expand_dims(X.flatten(), axis=1)\n",
    "X2_f= np.expand_dims(X2.flatten(), axis=1)\n",
    "Xlog_f= np.expand_dims(Xlog.flatten(), axis=1)\n",
    "d_f= np.expand_dims(d.flatten(), axis=1)\n",
    "ae_f=  np.expand_dims(amplitude_envelope.flatten(), axis=1)\n",
    "aed1_f=  np.expand_dims(envelope_derv1.flatten(), axis=1)\n",
    "aed2_f=  np.expand_dims(envelope_derv2.flatten(), axis=1)\n",
    "ip_f=  np.expand_dims(instantaneous_phase.flatten(), axis=1)\n",
    "if_f=  np.expand_dims(instantaneous_frequency.flatten(), axis=1)\n",
    "y_f= (y.flatten()).astype(np.int8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "* Need to merge test data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca1 = PCA()\n",
    "Xpc= pca1.fit_transform(np.hstack((X_f, X2_f, Xlog_f, ae_f, aed1_f, aed2_f, ip_f, if_f))).astype(np.float32)\n",
    "pca1.explained_variance_ratio_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca2 = PCA()\n",
    "XBVpc= pca2.fit_transform(XBV).astype(np.float32)\n",
    "pca2.explained_variance_ratio_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca3 = PCA()\n",
    "XBHpc= pca3.fit_transform(XBH).astype(np.float32)\n",
    "pca3.explained_variance_ratio_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "enc = KBinsDiscretizer(n_bins=100, encode='ordinal')\n",
    "X_binned = enc.fit_transform(Xpc[:, 0].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for q in range(100):\n",
    "    #z= X_binned[X_binned==q]\n",
    "    z1= y_f.reshape(-1,1)[X_binned==q]\n",
    "    plt.hist(z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne5= TSNE(n_components=3, perplexity= 5, learning_rate= 300, \n",
    "           n_iter= 250, n_iter_without_progress= 5, \n",
    "           random_state= 0, verbose= 5)\n",
    "tsne60= TSNE(n_components=3, perplexity= 60, learning_rate= 300, \n",
    "           n_iter= 1000, n_iter_without_progress= 5, \n",
    "           random_state= 0, verbose= 5)\n",
    "tsne100= TSNE(n_components=3, perplexity= 100, learning_rate= 300, \n",
    "           n_iter= 250, n_iter_without_progress= 5, \n",
    "           random_state= 0, verbose= 5)\n",
    "\n",
    "\n",
    "#data= np.hstack((d_f[4*63*63:7*63*63, :], Xpc[4*63*63:7*63*63, :]))\n",
    "\n",
    "Xsne5= tsne5.fit_transform(data)\n",
    "#Xsne60= tsne60.fit_transform(data)\n",
    "#Xsne100= tsne100.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "plt.scatter(Xsne5[:, 1], Xsne5[:, 2], s= 0.7, c=y_f[4*63*63:7*63*63]);plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "plt.scatter(Xsne100[:, 0], Xsne100[:, 1], s= 0.7, c=y_f[4*63*63:7*63*63]);plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "plt.scatter(Xsne30[:, 1], Xsne30[:, 2], s= 0.7, c=y_f[4*63*63:7*63*63]);plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN features\n",
    "Mean & Std of distances for datasets:\n",
    "* All\n",
    "* Data equals 1.\n",
    "* Data equals 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data= np.hstack((d_f, Xpc))\n",
    "scaler = MinMaxScaler()\n",
    "data= scaler.fit_transform(data)\n",
    "\n",
    "nj= 13\n",
    "knn_model= KNeighborsRegressor(n_neighbors=21, n_jobs= nj)\n",
    "\n",
    "knn_model.fit(data, y_f)\n",
    "dist, _= knn_model.kneighbors(data)\n",
    "dist= dist[:, 1:]\n",
    "print('Done')\n",
    "\n",
    "knn_model= KNeighborsRegressor(n_neighbors=11, n_jobs= nj)\n",
    "\n",
    "data_knn= data[y_f==1]\n",
    "y_f_knn= y_f[y_f==1]\n",
    "knn_model.fit(data_knn, y_f_knn)\n",
    "dist1, _= knn_model.kneighbors(data)\n",
    "dist1= dist1[:, 1:]\n",
    "print('Done')\n",
    "\n",
    "data_knn= data[y_f==0]\n",
    "y_f_knn= y_f[y_f==0]\n",
    "knn_model.fit(data_knn, y_f_knn)\n",
    "dist0, _= knn_model.kneighbors(data)\n",
    "dist0= dist0[:, 1:]\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "KNN= []\n",
    "dm1= dist1.mean(axis=1)\n",
    "ds1= dist1.std(axis=1)\n",
    "dm0= dist0.mean(axis=1)\n",
    "ds0= dist0.std(axis=1)\n",
    "\n",
    "KNN.append(dm1)\n",
    "KNN.append(ds1)\n",
    "KNN.append(dm0)\n",
    "KNN.append(ds0)\n",
    "\n",
    "KNN.append(dm0 - dm1)\n",
    "KNN.append(dm0 / dm1)\n",
    "KNN.append(np.log1p(dm0 / dm1))\n",
    "KNN.append(ds0 - ds1)\n",
    "for w in [3, 10, 20]:\n",
    "    dm= dist[:, :w].mean(axis=1)\n",
    "    KNN.append(dm)\n",
    "    KNN.append(dm - dm1)\n",
    "    KNN.append(dm - dm0)\n",
    "    #KNN.append(dm/dm1)\n",
    "    #KNN.append(dm/dm0)\n",
    "    \n",
    "    ds= dist[:, :w].std(axis=1)\n",
    "    KNN.append(ds)\n",
    "    KNN.append(ds - ds1)\n",
    "    KNN.append(ds - ds0)\n",
    "    #KNN.append(ds1/ds)\n",
    "    #KNN.append(ds0/ds)\n",
    "    \n",
    "\n",
    "KNN= np.array(KNN).T\n",
    "KNN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(KNN.shape[1]):\n",
    "    print(np.isnan(KNN[:,i]).sum())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Modeling\n",
    "* Stacking.\n",
    "* Normilizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_modeling= np.hstack((d_f, Xpc, XBVpc, XBHpc, KNN)).astype(np.float32)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_modeling= scaler.fit_transform(X_modeling).astype(np.float32)\n",
    "Xpc= scaler.fit_transform(Xpc).astype(np.float32)\n",
    "XBVpc= scaler.fit_transform(XBVpc).astype(np.float32)\n",
    "XBHpc= scaler.fit_transform(XBHpc).astype(np.float32)\n",
    "KNN= scaler.fit_transform(KNN).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "3 levels of modeling for various datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "level1= []\n",
    "\n",
    "lgb_params= {'feature_fraction': 0.7,'metric': 'rmse', 'nthread':12, 'min_data_in_leaf': 2**7, \n",
    "                  'bagging_fraction': 0.7, 'learning_rate': 0.05, 'objective': 'rmse',\n",
    "                  'bagging_seed': 2**5, 'num_leaves': 2**11,'bagging_freq':1}\n",
    "\n",
    "for level1_X in [X_modeling, X_modeling[:, :61], X_modeling[:, :21], Xpc, XBVpc, XBHpc, KNN]:#, Stas\n",
    "    \n",
    "    #print('CAT')\n",
    "    #level1_CATmodel = CatBoostRegressor(iterations=200, learning_rate=0.05, \n",
    "     #                                   min_data_in_leaf= 2**7, depth=5, \n",
    "      #                                  random_seed= 0, verbose= 50)\n",
    "    #level1_CATmodel.fit(level1_X, y_f)\n",
    "    #level1_pCAT= level1_CATmodel.predict(level1_X)\n",
    "    #level1.append(level1_pCAT)\n",
    "\n",
    "    print('LGB')\n",
    "    level1_LGBmodel = lgb.train(lgb_params, lgb.Dataset(level1_X, label=y_f), 50)\n",
    "    p= level1_LGBmodel.predict(level1_X)\n",
    "    error= y_f-p\n",
    "    weight= y_f-p + 1\n",
    "    \n",
    "    print('LGB_error')\n",
    "    lgb_params= {'feature_fraction': 0.7,'metric': 'rmse', 'nthread':12, 'min_data_in_leaf': 2**7, \n",
    "                  'bagging_fraction': 0.7, 'learning_rate': 0.05, 'objective': 'rmse',\n",
    "                  'bagging_seed': 2**5, 'num_leaves': 2**11,'bagging_freq':1,'verbosity': 50}\n",
    "    \n",
    "    level1_LGBmodel = lgb.train(lgb_params, lgb.Dataset(level1_X, label=y_f, weight=weight), 300)\n",
    "    p1= level1_LGBmodel.predict(level1_X)\n",
    "    error1= y_f-p1\n",
    "    \n",
    "    print('LGB_error1')\n",
    "    level1_LGBmodel = lgb.train(lgb_params, lgb.Dataset(level1_X, label=error1), 300)\n",
    "    p2= level1_LGBmodel.predict(level1_X)\n",
    "    \n",
    "    level1.append(p1 + p2)\n",
    "    ax = lgb.plot_importance(level1_LGBmodel, figsize=(12, 30));plt.show()\n",
    "    \n",
    "    #print('BR')\n",
    "    #level1_X= np.hstack((level1_X, np.ones((level1_X.shape[0], 1))))\n",
    "    #level1_BRmodel= BayesianRidge()\n",
    "    #level1_BRmodel.fit(level1_X, y_f)\n",
    "    #level1_pBR= level1_BRmodel.predict(level1_X)\n",
    "    #level1.append(level1_pBR)\n",
    "    \n",
    "level1= np.array(level1).T \n",
    "level1.max(axis=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "level= level1\n",
    "for col in range(level.shape[1]):\n",
    "    dice= []\n",
    "    thresholds= np.arange(0.1, level[:, col].max(), 0.01)\n",
    "    for threshold in thresholds:\n",
    "        p_test= np.zeros((level.shape[0], 1))\n",
    "\n",
    "        p_test[level[:, col] > threshold]= 1\n",
    "        p_test= p_test.astype(np.float32)\n",
    "        y_true= y_f.astype(np.float32)\n",
    "\n",
    "        dice.append(dice_coef1(y_true, p_test.squeeze()))\n",
    "    plt.plot(thresholds, dice)\n",
    "    max_dice= np.array(dice).max()\n",
    "    best_th= thresholds[np.argmax(np.array(dice))]\n",
    "    print('max dice is ' + str(max_dice) + ' at threshold ' + str(best_th))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_level2= level1\n",
    "X_level2.shape\n",
    "scaler1 = MinMaxScaler()\n",
    "X_level2= scaler1.fit_transform(X_level2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "level2= []\n",
    "\n",
    "print('BR')\n",
    "level2_BRmodel= BayesianRidge()\n",
    "level2_BRmodel.fit(X_level2, y_f)\n",
    "level2_pBR= level2_BRmodel.predict(X_level2)\n",
    "print(level2_pBR.max(), level2_BRmodel.coef_)\n",
    "level2.append(level2_pBR)\n",
    "\n",
    "print('KNN')\n",
    "level2_KNNmodel= KNeighborsRegressor(n_neighbors=3, n_jobs=12)\n",
    "level2_KNNmodel.fit(X_level2, y_f)\n",
    "level2_pKNN= level2_KNNmodel.predict(X_level2)\n",
    "level2.append(level2_pKNN)\n",
    "\n",
    "print('LGB')\n",
    "lgb_params= {'feature_fraction': 0.7,'metric': 'rmse', 'nthread':12, 'min_data_in_leaf': 2**7, \n",
    "              'bagging_fraction': 0.7, 'learning_rate': 0.05, 'objective': 'rmse',\n",
    "              'bagging_seed': 2**5, 'num_leaves': 2**11,'bagging_freq':1,'verbosity': 50}\n",
    "\n",
    "level2_LGBmodel = lgb.train(lgb_params, lgb.Dataset(X_level2, label=y_f), 50)\n",
    "p= level2_LGBmodel.predict(X_level2)\n",
    "error= y_f-p + 1\n",
    "\n",
    "print('LGB_error')\n",
    "level2_LGBmodel = lgb.train(lgb_params, lgb.Dataset(X_level2, label=y_f, weight=error), 300)\n",
    "p1= level2_LGBmodel.predict(X_level2)\n",
    "error1= y_f-p1\n",
    "\n",
    "print('LGB_error1')\n",
    "level2_LGBmodel = lgb.train(lgb_params, lgb.Dataset(X_level2, label=error1), 300)\n",
    "p2= level2_LGBmodel.predict(X_level2)\n",
    "\n",
    "level2.append(p1 + p2)\n",
    "#ax = lgb.plot_importance(level2_LGBmodel, figsize=(12, 30));plt.show()\n",
    "\n",
    "level2= np.array(level2).T \n",
    "level2.max(axis=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "level= level2\n",
    "for col in range(level.shape[1]): \n",
    "    dice= []\n",
    "    thresholds= np.arange(0.1, level[:, col].max(), 0.01)\n",
    "    for threshold in thresholds:\n",
    "        p_test= np.zeros((level.shape[0], 1))\n",
    "\n",
    "        p_test[level[:, col] > threshold]= 1\n",
    "        p_test= p_test.astype(np.float32)\n",
    "        y_true= y_f.astype(np.float32)\n",
    "\n",
    "        dice.append(dice_coef1(y_true, p_test.squeeze()))\n",
    "    plt.plot(thresholds, dice)\n",
    "    max_dice= np.array(dice).max()\n",
    "    best_th= thresholds[np.argmax(np.array(dice))]\n",
    "    print('max dice is ' + str(max_dice) + 'at threshold ' + str(best_th))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler2 = MinMaxScaler()\n",
    "X_level3= scaler2.fit_transform(level2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "level3_BRmodel= BayesianRidge()\n",
    "level3_BRmodel.fit(X_level3, y_f)\n",
    "level3_pBR= level3_BRmodel.predict(X_level3)\n",
    "print(level3_pBR.max(), level3_BRmodel.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "level= level3_pBR\n",
    "\n",
    "dice= []\n",
    "thresholds= np.arange(0.1, level.max(), 0.01)\n",
    "for threshold in thresholds:\n",
    "    p_test= np.zeros((level.shape))\n",
    "\n",
    "    p_test[level > threshold]= 1\n",
    "    p_test= p_test.astype(np.float32)\n",
    "    y_true= y_f.astype(np.float32)\n",
    "\n",
    "    dice.append(dice_coef1(y_true, p_test.squeeze()))\n",
    "plt.plot(thresholds, dice)\n",
    "max_dice= np.array(dice).max()\n",
    "best_th= thresholds[np.argmax(np.array(dice))]\n",
    "print('max dice is ' + str(max_dice) + 'at threshold ' + str(best_th))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_final= np.zeros((level3_pBR.shape))\n",
    "p_final[level3_pBR > best_th]= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imp= p_final.reshape((-1, 63, 63))\n",
    "y_c= y_true.reshape((-1, 63, 63))\n",
    "\n",
    "for i in np.arange(0, len(X), 5):\n",
    "    print(i)\n",
    "    show_im(X[i], y_c[i], imp[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Processing\n",
    "Smoothing noise and holes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Irosion/Dilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im= []\n",
    "for img in imp: \n",
    "    kernel2 = np.ones((2,2), np.uint8)\n",
    "    kernel3 = np.ones((3,3), np.uint8)\n",
    "    img_e= cv2.erode(img, kernel2, iterations=1)\n",
    "    img_d= cv2.dilate(img_e, kernel3, iterations=1)\n",
    "    img_d= cv2.dilate(img_d, kernel2, iterations=1)   \n",
    "    im.append(img_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in np.arange(0, len(X), 5):\n",
    "    print(i)\n",
    "    show_im( y_c[i], im[i], imp[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "level= (np.array(im).flatten()).astype(np.int8)\n",
    "\n",
    "dice= []\n",
    "thresholds= np.arange(0.1, level.max(), 0.01)\n",
    "for threshold in thresholds:\n",
    "    p_test= np.zeros((level.shape))\n",
    "\n",
    "    p_test[level > threshold]= 1\n",
    "    p_test= p_test.astype(np.float32)\n",
    "    y_true= y_f.astype(np.float32)\n",
    "\n",
    "    dice.append(dice_coef1(y_true, p_test.squeeze()))\n",
    "plt.plot(thresholds, dice)\n",
    "max_dice= np.array(dice).max()\n",
    "best_th= thresholds[np.argmax(np.array(dice))]\n",
    "print('max dice is ' + str(max_dice) + 'at threshold ' + str(best_th))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_img = Input(shape=(63, 63, 1))\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_in= p_final.reshape((-1, 63 * 63))\n",
    "autoencoder.fit(y_in, y,\n",
    "                epochs=100,\n",
    "                batch_size=128,\n",
    "                shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
