{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "\n",
    "from scipy.signal import hilbert\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "from skimage.morphology import square, dilation, disk\n",
    "from skimage.feature import canny\n",
    "from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,denoise_wavelet, estimate_sigma)\n",
    "from skimage.filters import median\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.color import rgb2gray\n",
    "import cv2\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge, Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "#from Networks import *\n",
    "#from Comp2_func import *\n",
    "#from IOU import *\n",
    "\n",
    "#from datetime import datetime\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im_load():\n",
    "    img_path= 'train_data/train_images/'\n",
    "    msk_path= 'train_data/train_masks/'\n",
    "    imx_bank= []\n",
    "    imy_bank= []\n",
    "    depths= []\n",
    "    depth_data= pd.read_csv('depths.csv')\n",
    "    for root, dirs, files in os.walk(img_path + '.'):  \n",
    "        for file_name in files:\n",
    "            imx= imread(img_path + file_name)\n",
    "            imy= imread(msk_path + file_name)\n",
    "            depth= depth_data['z'][depth_data['id']==file_name.split('.')[0]]\n",
    "\n",
    "            imx= rgb2gray(imx)\n",
    "            imy= imy / 65535\n",
    "            if imx.max()>0:\n",
    "                \n",
    "                #imx= (imx - np.mean(imx)) / np.std(imx)\n",
    "                size= (63, 63)\n",
    "                imx= cv2.resize(imx, size)\n",
    "                imy= cv2.resize(imy, size)\n",
    "                \n",
    "                imx_bank.append(imx)\n",
    "                imy_bank.append(imy)\n",
    "                depths.append(depth.values * np.ones((63*63, )).astype(int))  \n",
    "\n",
    "    imx_bank= np.array(imx_bank)\n",
    "    imy_bank= np.array(imy_bank)\n",
    "    depths= np.array(depths)\n",
    "    return imx_bank, imy_bank, depths\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "\n",
    "def BP_filter(data, lowcut, highcut, fs, order, axis):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    if axis=='vertical':\n",
    "        data= np.rot90(data)\n",
    "    y = lfilter(b, a, data)\n",
    "    if axis== 'vertical':\n",
    "        y= np.rot90(y, k=1, axes=(1,0))\n",
    "    return y\n",
    "\n",
    "def BP_filter90(data, lowcut, highcut, fs, order):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    #data= np.rot90(data)\n",
    "    y = lfilter(b, a, data)\n",
    "    #y= np.rot90(y, k=1, axes=(1,0))\n",
    "    return y\n",
    "\n",
    "def bp(X, lowcut, highcut, order, axis):\n",
    "    imx_bank= []\n",
    "    for imx in X:\n",
    "        bp= BP_filter(imx, lowcut=lowcut, highcut=highcut, fs=1/0.004, order=order, axis=axis)\n",
    "        imx_bank.append(bp/1.3)\n",
    "    imx_bank= np.array(imx_bank)\n",
    "    return imx_bank\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1 - dice_coef(y_true, y_pred)\n",
    "\n",
    "def binary_dice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) + (1 - dice_coef(y_true, y_pred))\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    \"\"\"\n",
    "    Dice = (2*|X & Y|)/ (|X|+ |Y|)\n",
    "         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n",
    "    ref: https://arxiv.org/pdf/1606.04797v1.pdf\n",
    "    \"\"\"\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\n",
    "\n",
    "def dice_coef1(y_true, y_pred, smooth=1):\n",
    "    \"\"\"\n",
    "    Dice = (2*|X & Y|)/ (|X|+ |Y|)\n",
    "         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n",
    "    ref: https://arxiv.org/pdf/1606.04797v1.pdf\n",
    "    \"\"\"\n",
    "    intersection = np.sum(np.abs(y_true * y_pred), axis=-1)\n",
    "    return (2. * intersection + smooth) / (np.sum(np.square(y_true),-1) + np.sum(np.square(y_pred),-1) + smooth)\n",
    "\n",
    "\n",
    "# Show images\n",
    "def show_im(X, y, imp):\n",
    "    #imy= np.squeeze(y)\n",
    "    #imx_show= np.squeeze(X)\n",
    "    #imp= np.squeeze(imp, axis=[0, -1])\n",
    "\n",
    "    fig, (ax1, ax2, ax3)= plt.subplots(1, 3)\n",
    "    ax1.imshow(X)\n",
    "    ax2.imshow(y)\n",
    "    ax3.imshow(imp)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images\n",
      "(3920, 63, 63)\n"
     ]
    }
   ],
   "source": [
    "print('Loading images')\n",
    "X, y, d= im_load()\n",
    "X= X.astype(np.float32)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-liniar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2= (X**2).astype(np.float32)\n",
    "Xlog= np.log1p(X).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency BandPass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertical axis\n",
    "variations= []\n",
    "low= [0.1, 2, 5, 10, 20] \n",
    "high= [7, 10, 20, 30, 40]\n",
    "order= [2, 3, 5]\n",
    "variations_prod= list(product(*[low, high, order]))\n",
    "[variations.append(x) for x in variations_prod if x[1]>x[0]]\n",
    "\n",
    "XBV= np.zeros((X.shape[0] * X.shape[1] * X.shape[2], len(variations)))\n",
    "for i, variation in enumerate(variations): \n",
    "    XBV[:, i]= bp(X, lowcut=variation[0], highcut=variation[1], order=variation[2], axis='vertical').flatten() #reshape((X.shape[0], X.shape[1] * X.shape[2] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal axis\n",
    "variations= []\n",
    "low= [0.1, 2, 5, 10, 20] \n",
    "high= [7, 10, 20, 30, 40]\n",
    "order= [2, 3, 5]\n",
    "variations_prod= list(product(*[low, high, order]))\n",
    "[variations.append(x) for x in variations_prod if x[1]>x[0]]\n",
    "\n",
    "XBH= np.zeros((X.shape[0] * X.shape[1] * X.shape[2], len(variations)))\n",
    "for i, variation in enumerate(variations): \n",
    "    XBH[:, i]= bp(X, lowcut=variation[0], highcut=variation[1], order=variation[2], axis='horizontal').flatten()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analitic Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs= 1/0.004\n",
    "analytic_signal = hilbert(X)\n",
    "\n",
    "amplitude_envelope = np.abs(analytic_signal)\n",
    "envelope_derv1= np.insert((np.diff(amplitude_envelope)), 0, 0, axis=-1)\n",
    "envelope_derv2= np.insert((np.diff(envelope_derv1)),0, 0, axis=-1)\n",
    "instantaneous_phase = np.unwrap(np.angle(analytic_signal))\n",
    "instantaneous_frequency = np.insert((np.diff(instantaneous_phase) / (2.0*np.pi) * fs),0, 0, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n in np.arange(0, len(X), 1):\n",
    "    print(n)\n",
    "    show_im(X[n], envelope_derv1[n], envelope_derv2[n])\n",
    "    show_im(amplitude_envelope[n], instantaneous_phase[n], instantaneous_frequency[n])\n",
    "    show_im(y[n], X2[n], Xlog[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_f= np.expand_dims(X.flatten(), axis=1)\n",
    "X2_f= np.expand_dims(X2.flatten(), axis=1)\n",
    "Xlog_f= np.expand_dims(Xlog.flatten(), axis=1)\n",
    "d_f= np.expand_dims(d.flatten(), axis=1)\n",
    "ae_f=  np.expand_dims(amplitude_envelope.flatten(), axis=1)\n",
    "aed1_f=  np.expand_dims(envelope_derv1.flatten(), axis=1)\n",
    "aed2_f=  np.expand_dims(envelope_derv2.flatten(), axis=1)\n",
    "ip_f=  np.expand_dims(instantaneous_phase.flatten(), axis=1)\n",
    "if_f=  np.expand_dims(instantaneous_frequency.flatten(), axis=1)\n",
    "y_f= (y.flatten()).astype(np.int8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "* Need to merge test data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.96838581e-01, 1.75138007e-03, 1.22037141e-03, 1.45372145e-04,\n",
       "       1.97878556e-05, 1.44052465e-05, 1.00932509e-05, 8.57259470e-09])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca1 = PCA()\n",
    "Xpc= pca1.fit_transform(np.hstack((X_f, X2_f, Xlog_f, ae_f, aed1_f, aed2_f, ip_f, if_f))).astype(np.float32)\n",
    "pca1.explained_variance_ratio_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xpc1= Xpc[:, 0].reshape(-1, 63, 63)\n",
    "Xpc2= Xpc[:, 1].reshape(-1, 63, 63)\n",
    "Xpc3= Xpc[:, 2].reshape(-1, 63, 63)\n",
    "Xpc4= Xpc[:, 3].reshape(-1, 63, 63)\n",
    "Xpc5= Xpc[:, 4].reshape(-1, 63, 63)\n",
    "Xpc6= Xpc[:, 5].reshape(-1, 63, 63)\n",
    "Xpc7= Xpc[:, 6].reshape(-1, 63, 63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=15\n",
    "show_im(X[n], Xpc1[n], Xpc2[n])\n",
    "show_im(Xpc3[n], Xpc4[n], Xpc5[n])\n",
    "show_im(Xpc6[n], Xpc7[n], y[n])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.64064524e-01, 2.40532070e-01, 1.64632955e-01, 9.59964170e-02,\n",
       "       4.73215540e-02, 3.42361093e-02, 1.91960204e-02, 1.31947027e-02,\n",
       "       8.42757475e-03, 4.59525829e-03, 4.10732152e-03, 2.23285485e-03,\n",
       "       8.38500945e-04, 3.27341237e-04, 1.91655351e-04, 7.40147537e-05,\n",
       "       2.01859408e-05, 7.14954702e-06, 2.58553819e-06, 6.23720955e-07,\n",
       "       4.91784992e-07, 4.43824948e-08, 3.76721902e-08, 6.34564721e-09,\n",
       "       6.00475535e-10, 4.47605092e-10, 9.58176246e-12, 4.09413331e-12,\n",
       "       4.66906681e-13, 1.71549642e-13, 8.24679995e-14, 1.85238767e-14,\n",
       "       4.83110603e-15, 3.81464660e-15, 2.06888187e-15, 1.51960637e-15,\n",
       "       5.38509860e-17, 3.85513721e-17, 2.16026925e-17, 3.35244420e-18,\n",
       "       9.50424857e-19, 4.13445128e-19, 2.57098374e-19, 1.57545848e-19,\n",
       "       1.03155260e-19, 1.81482876e-20, 5.97639059e-21, 4.32012052e-21,\n",
       "       1.79281422e-21, 1.64925276e-22, 1.46470962e-22, 7.63091634e-23,\n",
       "       3.26782799e-23, 3.95450030e-24, 1.30575932e-24, 7.12828664e-25,\n",
       "       1.20183177e-25, 9.76325537e-26, 4.03070765e-26, 3.24022357e-27])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca2 = PCA()\n",
    "XBVpc= pca2.fit_transform(XBV).astype(np.float32)\n",
    "pca2.explained_variance_ratio_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BPVpc1= XBVpc[:, 0].reshape(-1, 63, 63)\n",
    "BPVpc2= XBVpc[:, 1].reshape(-1, 63, 63)\n",
    "BPVpc3= XBVpc[:, 2].reshape(-1, 63, 63)\n",
    "BPVpc4= XBVpc[:, 3].reshape(-1, 63, 63)\n",
    "BPVpc5= XBVpc[:, 4].reshape(-1, 63, 63)\n",
    "BPVpc6= XBVpc[:, 5].reshape(-1, 63, 63)\n",
    "BPVpc7= XBVpc[:, 6].reshape(-1, 63, 63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=15\n",
    "show_im(X[n], BPVpc1[n], BPVpc2[n])\n",
    "show_im(BPVpc3[n], BPVpc4[n], BPVpc5[n])\n",
    "show_im(BPVpc6[n], BPVpc7[n], y[n])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.71556074e-01, 2.74417926e-01, 1.48545107e-01, 8.61069292e-02,\n",
       "       4.47241329e-02, 2.80412703e-02, 1.83580507e-02, 1.13102744e-02,\n",
       "       6.46218315e-03, 3.96266616e-03, 3.49337644e-03, 1.82044285e-03,\n",
       "       6.45422972e-04, 3.13963956e-04, 1.55251920e-04, 6.15203861e-05,\n",
       "       1.61481228e-05, 5.91883098e-06, 2.33154871e-06, 5.05696669e-07,\n",
       "       4.24145348e-07, 3.86236942e-08, 3.32193023e-08, 5.91143942e-09,\n",
       "       5.56719128e-10, 3.87860498e-10, 8.40367533e-12, 3.86346436e-12,\n",
       "       4.27182477e-13, 1.77899745e-13, 8.44757301e-14, 1.72606605e-14,\n",
       "       4.97937104e-15, 3.59221944e-15, 2.09926394e-15, 1.56648652e-15,\n",
       "       5.18460434e-17, 3.81790234e-17, 2.18023185e-17, 3.17116015e-18,\n",
       "       9.86098750e-19, 4.22116103e-19, 2.56850035e-19, 1.51168416e-19,\n",
       "       1.03934287e-19, 1.76421551e-20, 6.23045190e-21, 4.19587417e-21,\n",
       "       1.86997299e-21, 1.65829821e-22, 1.45124541e-22, 7.78544311e-23,\n",
       "       3.37531911e-23, 3.81949414e-24, 1.29873447e-24, 7.15518332e-25,\n",
       "       1.22126202e-25, 9.93952757e-26, 4.05826932e-26, 3.39197632e-27])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca3 = PCA()\n",
    "XBHpc= pca3.fit_transform(XBH).astype(np.float32)\n",
    "pca3.explained_variance_ratio_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Median Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "enc = KBinsDiscretizer(n_bins=100, encode='ordinal')\n",
    "X_binned = enc.fit_transform(Xpc[:, 0].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for q in range(100):\n",
    "    #z= X_binned[X_binned==q]\n",
    "    z1= y_f.reshape(-1,1)[X_binned==q]\n",
    "    plt.hist(z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne5= TSNE(n_components=3, perplexity= 5, learning_rate= 300, \n",
    "           n_iter= 250, n_iter_without_progress= 5, \n",
    "           random_state= 0, verbose= 5)\n",
    "tsne60= TSNE(n_components=3, perplexity= 60, learning_rate= 300, \n",
    "           n_iter= 1000, n_iter_without_progress= 5, \n",
    "           random_state= 0, verbose= 5)\n",
    "tsne100= TSNE(n_components=3, perplexity= 100, learning_rate= 300, \n",
    "           n_iter= 250, n_iter_without_progress= 5, \n",
    "           random_state= 0, verbose= 5)\n",
    "\n",
    "\n",
    "#data= np.hstack((d_f[4*63*63:7*63*63, :], Xpc[4*63*63:7*63*63, :]))\n",
    "\n",
    "Xsne5= tsne5.fit_transform(data)\n",
    "#Xsne60= tsne60.fit_transform(data)\n",
    "#Xsne100= tsne100.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "plt.scatter(Xsne5[:, 1], Xsne5[:, 2], s= 0.7, c=y_f[4*63*63:7*63*63]);plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "plt.scatter(Xsne100[:, 0], Xsne100[:, 1], s= 0.7, c=y_f[4*63*63:7*63*63]);plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "plt.scatter(Xsne30[:, 1], Xsne30[:, 2], s= 0.7, c=y_f[4*63*63:7*63*63]);plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN features\n",
    "* Mean dist from 10 1.\n",
    "* Mean dist from 10 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15558480, 19)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= np.hstack((d_f, Xpc))\n",
    "scaler = MinMaxScaler()\n",
    "data= scaler.fit_transform(data)\n",
    "\n",
    "knn_model= KNeighborsRegressor(n_neighbors=21, n_jobs=12)\n",
    "\n",
    "#ynn= y_f[4*63*63:7*63*63]\n",
    "knn_model.fit(data, y_f)\n",
    "dist, _= knn_model.kneighbors(data)\n",
    "dist= dist[:, 1:]\n",
    "\n",
    "knn_model= KNeighborsRegressor(n_neighbors=11, n_jobs=12)\n",
    "\n",
    "data1= data[y_f==1]\n",
    "knn_model.fit(data1, y_f)\n",
    "dist1, _= knn_model.kneighbors(data1)\n",
    "dist1= dist1[:, 1:]\n",
    "\n",
    "data0= data[y_f==0]\n",
    "knn_model.fit(data0, y_f)\n",
    "dist0, _= knn_model.kneighbors(data0)\n",
    "dist0= dist0[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15558480, 6)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN= []\n",
    "for w in [3, 9, 19]:\n",
    "    KNN.append(dist[:, :w].mean(axis=1))\n",
    "    KNN.append(dist[:, :w].std(axis=1))\n",
    "KNN.append(dist1.mean(axis=1))\n",
    "KNN.append(dist1.std(axis=1))\n",
    "KNN.append(dist0.mean(axis=1))\n",
    "KNN.append(dist0.std(axis=1))\n",
    "    \n",
    "\n",
    "KNN= np.array(Stas).T\n",
    "KNN.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_stacked= np.hstack((X_f, X2_f, Xlog_f, d_f, XBV, XBH, ae_f, aed1_f, aed2_f, ip_f, if_f))\n",
    "\n",
    "X_modeling= np.hstack((d_f, Xpc, XBVpc, XBHpc, KNN)).astype(np.float32)#, Stas\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_modeling= scaler.fit_transform(X_modeling).astype(np.float32)\n",
    "#print(X_modeling.max(), X_modeling.shape)\n",
    "\n",
    "Xpc= scaler.fit_transform(Xpc).astype(np.float32)\n",
    "XBVpc= scaler.fit_transform(XBVpc).astype(np.float32)\n",
    "XBHpc= scaler.fit_transform(XBHpc).astype(np.float32)\n",
    "KNN= scaler.fit_transform(KNN).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "level1= []\n",
    "\n",
    "lgb_params= {'feature_fraction': 0.7,'metric': 'rmse', 'nthread':12, 'min_data_in_leaf': 2**7, \n",
    "                  'bagging_fraction': 0.7, 'learning_rate': 0.05, 'objective': 'rmse',\n",
    "                  'bagging_seed': 2**5, 'num_leaves': 2**11,'bagging_freq':1}\n",
    "\n",
    "for level1_X in [X_modeling, X_modeling[:, :61], X_modeling[:, :21], Xpc, XBVpc, XBHpc, KNN]:#, Stas\n",
    "    \n",
    "    #print('CAT')\n",
    "    #level1_CATmodel = CatBoostRegressor(iterations=200, learning_rate=0.05, \n",
    "     #                                   min_data_in_leaf= 2**7, depth=5, \n",
    "      #                                  random_seed= 0, verbose= 50)\n",
    "    #level1_CATmodel.fit(level1_X, y_f)\n",
    "    #level1_pCAT= level1_CATmodel.predict(level1_X)\n",
    "    #level1.append(level1_pCAT)\n",
    "\n",
    "    print('LGB')\n",
    "    level1_LGBmodel = lgb.train(lgb_params, lgb.Dataset(level1_X, label=y_f), 50)\n",
    "    p= level1_LGBmodel.predict(level1_X)\n",
    "    error= y_f-p + 1\n",
    "    \n",
    "    print('LGB_error')\n",
    "    lgb_params= {'feature_fraction': 0.7,'metric': 'rmse', 'nthread':12, 'min_data_in_leaf': 2**7, \n",
    "                  'bagging_fraction': 0.7, 'learning_rate': 0.05, 'objective': 'rmse',\n",
    "                  'bagging_seed': 2**5, 'num_leaves': 2**11,'bagging_freq':1,'verbosity': 50}\n",
    "    \n",
    "    level1_LGBmodel = lgb.train(lgb_params, lgb.Dataset(level1_X, label=y_f, weight=error), 300)\n",
    "    p1= level1_LGBmodel.predict(level1_X)\n",
    "    error1= y_f-p1\n",
    "    \n",
    "    print('LGB_error1')\n",
    "    level1_LGBmodel = lgb.train(lgb_params, lgb.Dataset(level1_X, label=error1), 300)\n",
    "    p2= level1_LGBmodel.predict(level1_X)\n",
    "    \n",
    "    level1.append(p1 + p2)\n",
    "    ax = lgb.plot_importance(level1_LGBmodel, figsize=(12, 30));plt.show()\n",
    "    \n",
    "    #print('BR')\n",
    "    #level1_X= np.hstack((level1_X, np.ones((level1_X.shape[0], 1))))\n",
    "    #level1_BRmodel= BayesianRidge()\n",
    "    #level1_BRmodel.fit(level1_X, y_f)\n",
    "    #level1_pBR= level1_BRmodel.predict(level1_X)\n",
    "    #level1.append(level1_pBR)\n",
    "    \n",
    "level1= np.array(level1).T \n",
    "level1.max(axis=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level= level1\n",
    "for col in range(level.shape[1]):\n",
    "    dice= []\n",
    "    thresholds= np.arange(0.1, level[:, col].max(), 0.01)\n",
    "    for threshold in thresholds:\n",
    "        p_test= np.zeros((level.shape[0], 1))\n",
    "\n",
    "        p_test[level[:, col] > threshold]= 1\n",
    "        p_test= p_test.astype(np.float32)\n",
    "        y_true= y_f.astype(np.float32)\n",
    "\n",
    "        dice.append(dice_coef1(y_true, p_test.squeeze()))\n",
    "    plt.plot(thresholds, dice)\n",
    "    max_dice= np.array(dice).max()\n",
    "    best_th= thresholds[np.argmax(np.array(dice))]\n",
    "    print('max dice is ' + str(max_dice) + ' at threshold ' + str(best_th))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_level2= level1\n",
    "X_level2.shape\n",
    "scaler1 = MinMaxScaler()\n",
    "X_level2= scaler1.fit_transform(X_level2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "level2= []\n",
    "\n",
    "print('BR')\n",
    "level2_BRmodel= BayesianRidge()\n",
    "level2_BRmodel.fit(X_level2, y_f)\n",
    "level2_pBR= level2_BRmodel.predict(X_level2)\n",
    "print(level2_pBR.max(), level2_BRmodel.coef_)\n",
    "level2.append(level2_pBR)\n",
    "\n",
    "print('KNN')\n",
    "level2_KNNmodel= KNeighborsRegressor(n_neighbors=3, n_jobs=12)\n",
    "level2_KNNmodel.fit(X_level2, y_f)\n",
    "level2_pKNN= level2_KNNmodel.predict(X_level2)\n",
    "level2.append(level2_pKNN)\n",
    "\n",
    "print('LGB')\n",
    "lgb_params= {'feature_fraction': 0.7,'metric': 'rmse', 'nthread':12, 'min_data_in_leaf': 2**7, \n",
    "              'bagging_fraction': 0.7, 'learning_rate': 0.05, 'objective': 'rmse',\n",
    "              'bagging_seed': 2**5, 'num_leaves': 2**11,'bagging_freq':1,'verbosity': 50}\n",
    "\n",
    "level2_LGBmodel = lgb.train(lgb_params, lgb.Dataset(X_level2, label=y_f), 50)\n",
    "p= level2_LGBmodel.predict(X_level2)\n",
    "error= y_f-p + 1\n",
    "\n",
    "print('LGB_error')\n",
    "level2_LGBmodel = lgb.train(lgb_params, lgb.Dataset(X_level2, label=y_f, weight=error), 300)\n",
    "p1= level2_LGBmodel.predict(X_level2)\n",
    "error1= y_f-p1\n",
    "\n",
    "print('LGB_error1')\n",
    "level2_LGBmodel = lgb.train(lgb_params, lgb.Dataset(X_level2, label=error1), 300)\n",
    "p2= level2_LGBmodel.predict(X_level2)\n",
    "\n",
    "level2.append(p1 + p2)\n",
    "#ax = lgb.plot_importance(level2_LGBmodel, figsize=(12, 30));plt.show()\n",
    "\n",
    "level2= np.array(level2).T \n",
    "level2.max(axis=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "level= level2\n",
    "for col in range(level.shape[1]): \n",
    "    dice= []\n",
    "    thresholds= np.arange(0.1, level[:, col].max(), 0.01)\n",
    "    for threshold in thresholds:\n",
    "        p_test= np.zeros((level.shape[0], 1))\n",
    "\n",
    "        p_test[level[:, col] > threshold]= 1\n",
    "        p_test= p_test.astype(np.float32)\n",
    "        y_true= y_f.astype(np.float32)\n",
    "\n",
    "        dice.append(dice_coef1(y_true, p_test.squeeze()))\n",
    "    plt.plot(thresholds, dice)\n",
    "    max_dice= np.array(dice).max()\n",
    "    best_th= thresholds[np.argmax(np.array(dice))]\n",
    "    print('max dice is ' + str(max_dice) + 'at threshold ' + str(best_th))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler2 = MinMaxScaler()\n",
    "X_level3= scaler2.fit_transform(level2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0839021003186202 [-0.26679021  0.87742514  0.35105844]\n"
     ]
    }
   ],
   "source": [
    "level3_BRmodel= BayesianRidge()\n",
    "level3_BRmodel.fit(X_level3, y_f)\n",
    "level3_pBR= level3_BRmodel.predict(X_level3)\n",
    "print(level3_pBR.max(), level3_BRmodel.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "level= level3_pBR\n",
    "\n",
    "dice= []\n",
    "thresholds= np.arange(0.1, level.max(), 0.01)\n",
    "for threshold in thresholds:\n",
    "    p_test= np.zeros((level.shape))\n",
    "\n",
    "    p_test[level > threshold]= 1\n",
    "    p_test= p_test.astype(np.float32)\n",
    "    y_true= y_f.astype(np.float32)\n",
    "\n",
    "    dice.append(dice_coef1(y_true, p_test.squeeze()))\n",
    "plt.plot(thresholds, dice)\n",
    "max_dice= np.array(dice).max()\n",
    "best_th= thresholds[np.argmax(np.array(dice))]\n",
    "print('max dice is ' + str(max_dice) + 'at threshold ' + str(best_th))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_final= np.zeros((level3_pBR.shape))\n",
    "p_final[level3_pBR > best_th]= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imp= p_final.reshape((-1, 63, 63))\n",
    "y_c= y_true.reshape((-1, 63, 63))\n",
    "\n",
    "for i in np.arange(0, len(X), 5):\n",
    "    print(i)\n",
    "    show_im(X[i], y_c[i], imp[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Irosion/Dilation/AutoEncoder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
