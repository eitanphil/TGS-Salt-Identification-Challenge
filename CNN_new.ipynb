{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "\n",
    "from scipy.signal import hilbert\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "from skimage.morphology import square, dilation, disk\n",
    "from skimage.feature import canny\n",
    "from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,denoise_wavelet, estimate_sigma)\n",
    "from skimage.filters import median\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.color import rgb2gray\n",
    "import cv2\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge, Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Dropout, merge, Reshape, BatchNormalization, LeakyReLU,  Concatenate, Activation, Add, Conv2DTranspose, concatenate\n",
    "from keras import optimizers\n",
    "from keras import activations\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "\n",
    "#from Networks import *\n",
    "#from Comp2_func import *\n",
    "#from IOU import *\n",
    "\n",
    "#from datetime import datetime\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge, Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def im_load(size):\n",
    "    img_path= 'C:/Users/eitanp/Desktop/ML/Competition/Salt_Win/train_data/train_images/'\n",
    "    msk_path= 'C:/Users/eitanp/Desktop/ML/Competition/Salt_Win/train_data/train_masks/'\n",
    "    imx_bank= []\n",
    "    imy_bank= []\n",
    "    depths= []\n",
    "    depth_data= pd.read_csv('depths.csv')\n",
    "    for root, dirs, files in os.walk(img_path + '.'):  \n",
    "        for file_name in files:\n",
    "            imx= imread(img_path + file_name)\n",
    "            imy= imread(msk_path + file_name)\n",
    "            depth= depth_data['z'][depth_data['id']==file_name.split('.')[0]]\n",
    "\n",
    "            imx= rgb2gray(imx)\n",
    "            imy= imy / 65535\n",
    "            if imx.max()>0:\n",
    "                \n",
    "                #imx= (imx - np.mean(imx)) / np.std(imx)\n",
    "                #size= (63, 63)\n",
    "                imx= cv2.resize(imx, size)\n",
    "                imy= cv2.resize(imy, size)\n",
    "                \n",
    "                imx_bank.append(imx)\n",
    "                imy_bank.append(imy)\n",
    "                depths.append(depth.values * np.ones((size[0] * size[1], )).astype(int))  \n",
    "\n",
    "    imx_bank= np.array(imx_bank)\n",
    "    imy_bank= np.array(imy_bank)\n",
    "    depths= np.array(depths)\n",
    "    return imx_bank, imy_bank, depths\n",
    "\n",
    "def test_load(size):\n",
    "    img_path= 'C:/Users/eitanp/Desktop/ML/Competition/Salt_Win/test_images/'\n",
    "    imx_bank= []\n",
    "    names_bank= []\n",
    "    names0_bank= []\n",
    "    depths= []\n",
    "    depth_data= pd.read_csv('depths.csv')\n",
    "    for root, dirs, files in os.walk(img_path + '.'):  \n",
    "        for file_name in files:\n",
    "            imx= imread(img_path + file_name)\n",
    "           \n",
    "            imx= rgb2gray(imx)\n",
    "            #print(imx.shape)\n",
    "            if imx.max()>0:\n",
    "                imx= cv2.resize(imx, size)\n",
    "                #print(imx.shape)\n",
    "\n",
    "                depth= depth_data['z'][depth_data['id']==file_name.split('.')[0]]\n",
    "\n",
    "                names_bank.append(file_name.split('.')[0])\n",
    "\n",
    "                imx_bank.append(imx)\n",
    "                depths.append(depth.values * np.ones((size[0] * size[1], )).astype(int)) \n",
    "            else:\n",
    "                names0_bank.append(file_name.split('.')[0])\n",
    "    \n",
    "    imx_bank= np.array(imx_bank)\n",
    "    #print(imx_bank.shape)\n",
    "    \n",
    "    depths= np.array(depths)\n",
    "    return imx_bank, depths, names_bank, names0_bank\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "\n",
    "def BP_filter(data, lowcut, highcut, fs, order, axis):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    if axis=='vertical':\n",
    "        data= np.rot90(data)\n",
    "    y = lfilter(b, a, data)\n",
    "    if axis== 'vertical':\n",
    "        y= np.rot90(y, k=1, axes=(1,0))\n",
    "    return y\n",
    "\n",
    "def BP_filter90(data, lowcut, highcut, fs, order):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    #data= np.rot90(data)\n",
    "    y = lfilter(b, a, data)\n",
    "    #y= np.rot90(y, k=1, axes=(1,0))\n",
    "    return y\n",
    "\n",
    "def bp(X, lowcut, highcut, order, axis):\n",
    "    imx_bank= []\n",
    "    for imx in X:\n",
    "        bp= BP_filter(imx, lowcut=lowcut, highcut=highcut, fs=1/0.004, order=order, axis=axis)\n",
    "        imx_bank.append(bp/1.3)\n",
    "    imx_bank= np.array(imx_bank)\n",
    "    return imx_bank\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1 - dice_coef(y_true, y_pred)\n",
    "\n",
    "def binary_dice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) + (1 - dice_coef(y_true, y_pred))\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    \"\"\"\n",
    "    Dice = (2*|X & Y|)/ (|X|+ |Y|)\n",
    "         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n",
    "    ref: https://arxiv.org/pdf/1606.04797v1.pdf\n",
    "    \"\"\"\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\n",
    "\n",
    "def dice_coef1(y_true, y_pred, smooth=1):\n",
    "    \"\"\"\n",
    "    Dice = (2*|X & Y|)/ (|X|+ |Y|)\n",
    "         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n",
    "    ref: https://arxiv.org/pdf/1606.04797v1.pdf\n",
    "    \"\"\"\n",
    "    intersection = np.sum(np.abs(y_true * y_pred), axis=-1)\n",
    "    return (2. * intersection + smooth) / (np.sum(np.square(y_true),-1) + np.sum(np.square(y_pred),-1) + smooth)\n",
    "\n",
    "\n",
    "# Show images\n",
    "def show_im2(X, y):\n",
    "    #imy= np.squeeze(y)\n",
    "    #imx_show= np.squeeze(X)\n",
    "    #imp= np.squeeze(imp, axis=[0, -1])\n",
    "\n",
    "    fig, (ax1, ax2)= plt.subplots(1, 2)\n",
    "    ax1.imshow(X)\n",
    "    ax2.imshow(y)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_im3(X, y, imp):\n",
    "    #imy= np.squeeze(y)\n",
    "    #imx_show= np.squeeze(X)\n",
    "    #imp= np.squeeze(imp, axis=[0, -1])\n",
    "\n",
    "    fig, (ax1, ax2, ax3)= plt.subplots(1, 3)\n",
    "    ax1.imshow(X)\n",
    "    ax2.imshow(y)\n",
    "    ax3.imshow(imp)\n",
    "    plt.show()\n",
    "    \n",
    "def show_im4(X, y, imp, imp1):\n",
    "    #imy= np.squeeze(y)\n",
    "    #imx_show= np.squeeze(X)\n",
    "    #imp= np.squeeze(imp, axis=[0, -1])\n",
    "\n",
    "    fig, (ax1, ax2, ax3, ax4)= plt.subplots(1, 4)\n",
    "    ax1.imshow(X)\n",
    "    ax2.imshow(y)\n",
    "    ax3.imshow(imp)\n",
    "    ax4.imshow(imp1)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# Results\n",
    "def rle (img):\n",
    "    result=''\n",
    "    flat_img = img.flatten(order='F')\n",
    "    flat_img = np.where(flat_img > 0.5, 1, 0).astype(np.uint8)\n",
    "\n",
    "    starts = (flat_img == 1) & (np.insert(flat_img[:-1], 0, 0)==0)\n",
    "    ends = (flat_img == 1) & (np.insert(flat_img[1:], len(flat_img) - 1, 0)==0)\n",
    "    starts_ix = np.where(starts)[0]\n",
    "    ends_ix = np.where(ends)[0]\n",
    "    lengths = (ends_ix - starts_ix) + 1\n",
    "    for s, l in zip(starts_ix + 1, lengths):\n",
    "        result+= '{} {} '.format(s, l)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mid_path= 'Mid_data/'\n",
    "models_path= 'Models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size= (96, 96)\n",
    "size= (96, 96)\n",
    "print('Loading train images')\n",
    "X, y, d= im_load(size)\n",
    "X= X.astype(np.float32)\n",
    "print(X.shape)\n",
    "\n",
    "print('Loading test images')\n",
    "T, dt, names, names0= test_load(size)\n",
    "T= T.astype(np.float32)\n",
    "print(T.shape)\n",
    "#np.save(mid_path + 'X' + str(size[0]), X); np.save(mid_path + 'T' + str(size[0]), T)\n",
    "#np.save(mid_path + 'y' + str(size[0]), y)\n",
    "#np.save(mid_path + 'd_f' + str(size[0]), d.flatten())\n",
    "#np.save(mid_path + 'dt_f' + str(size[0]), dt.flatten())\n",
    "np.save(mid_path + 'names', names); np.save(mid_path + 'names0', names0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing data for QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x= X.reshape(-1, 96 * 96); t= T.reshape(-1, 96 * 96)\n",
    "Xn= (x- np.expand_dims(x.mean(axis=1), axis=1))/np.expand_dims(x.std(axis=1), axis=1); \n",
    "Tn= (t- np.expand_dims(t.mean(axis=1), axis=1))/np.expand_dims(t.std(axis=1), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=5\n",
    "lim_range= (-2, 2)\n",
    "fig, (ax1, ax2)= plt.subplots(1, 2)\n",
    "ax1.hist(X[n].flatten(), 100)\n",
    "ax1.set_xlim(lim_range)\n",
    "ax2.hist(Xn[n].flatten(), 100)\n",
    "ax2.set_xlim(lim_range)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AutoD1():\n",
    "    input_img = Input(shape=(96, 96, 1))\n",
    "    #lr= 0.5\n",
    "    #decay= 5e-5\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "    encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    #opt= optimizers.Adam(lr = lr, decay= decay)\n",
    "    autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "    return autoencoder\n",
    "\n",
    "def AutoD2():\n",
    "    input_img = Input(shape=(96, 96, 1))\n",
    "    #lr= 0.5\n",
    "    #decay= 5e-5\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    #opt= optimizers.Adam(lr = lr, decay= decay)\n",
    "    autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X= np.load(mid_path + 'X96.npy'); T= np.load(mid_path + 'T96.npy')\n",
    "y= np.load(mid_path + 'y96.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_in= X\n",
    "y_in= y_in.reshape((-1, 96 , 96, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 2 layers for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autoencoderD2E10= AutoD2()\n",
    "autoencoderD2E10.fit(y_in, y_in,\n",
    "                epochs=10,\n",
    "                batch_size=128,\n",
    "                shuffle=True)\n",
    "autoencoderD2E10.save(models_path + 'AutoD2E10.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predD2E10= autoencoderD2E10.predict(y_in)\n",
    "predD2E10= predD2E10.squeeze(axis= -1)\n",
    "\n",
    "y_inT= T.reshape((-1, 96 , 96, 1))\n",
    "predD2E10T= autoencoderD2E10.predict(y_inT)\n",
    "predD2E10T= predD2E10T.squeeze(axis= -1)\n",
    "\n",
    "np.save(mid_path + 'predD2E10', predD2E10); np.save(mid_path + 'predD2E10T', predD2E10T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(0, 300, 5):\n",
    "    print(i)\n",
    "    show_im4(X[i], predD2E10[i], X[i] - predD2E10[i], y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(0, 300, 5):\n",
    "    print(i)\n",
    "    show_im3( T[i], predD2E10T[i], T[i] - predD2E10T[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=45\n",
    "fig, (ax1, ax2)= plt.subplots(1, 2)\n",
    "ax1.hist(X[n].flatten(), 100)\n",
    "ax1.set_xlim(0,1)\n",
    "ax2.hist(predD2E10[n].flatten(), 100)\n",
    "ax2.set_xlim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 2 layers for 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autoencoderD2E50= AutoD2()\n",
    "autoencoderD2E50.fit(y_in, y_in,\n",
    "                epochs=50,\n",
    "                batch_size=128,\n",
    "                shuffle=True)\n",
    "autoencoderD2E50.save(models_path + 'AutoD2E50.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predD2E50= autoencoderD2E50.predict(y_in)\n",
    "predD2E50= predD2E50.squeeze(axis= -1)\n",
    "\n",
    "y_inT= T.reshape((-1, 96 , 96, 1))\n",
    "predD2E50T= autoencoderD2E50.predict(y_inT)\n",
    "predD2E50T= predD2E50T.squeeze(axis= -1)\n",
    "\n",
    "np.save(mid_path + 'predD2E50', predD2E50); np.save(mid_path + 'predD2E50T', predD2E50T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in np.arange(0, 300, 5):\n",
    "    print(i)\n",
    "    show_im4(X[i], predD2E50[i],  X[i] - predD2E50[i], y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(0, 300, 5):\n",
    "    print(i)\n",
    "    show_im3( T[i], predD2E50T[i], T[i] - predD2E50T[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 1 layer for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoderD1E10= AutoD1()\n",
    "autoencoderD1E10.fit(y_in, y_in,\n",
    "                epochs=10,\n",
    "                batch_size=128,\n",
    "                shuffle=True)\n",
    "autoencoderD1E10.save(models_path + 'AutoD1E10.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predD1E10= autoencoderD1E10.predict(y_in)\n",
    "predD1E10= predD1E10.squeeze(axis= -1)\n",
    "\n",
    "predD1E10T= autoencoderD1E10.predict(y_inT)\n",
    "predD1E10T= predD1E10T.squeeze(axis= -1)\n",
    "\n",
    "np.save(mid_path + 'predD1E10', predD1E10); np.save(mid_path + 'predD1E10T', predD1E10T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(0, 300, 5):\n",
    "    print(i)\n",
    "    show_im4(X[i], predD1E10[i],  X[i] - predD1E10[i], y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(0, 300, 5):\n",
    "    print(i)\n",
    "    show_im3(T[i], predD1E10T[i],  T[i] - predD1E10T[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x= pred1.reshape(-1, 96 * 96); t= pred1T.reshape(-1, 96 * 96)\n",
    "#Xn= (x- np.expand_dims(x.mean(axis=1), axis=1))/np.expand_dims(x.std(axis=1), axis=1); \n",
    "#Tn= (t- np.expand_dims(t.mean(axis=1), axis=1))/np.expand_dims(t.std(axis=1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=5\n",
    "lim_range= (0, 1)\n",
    "fig, (ax1, ax2, ax3)= plt.subplots(1, 3)\n",
    "ax1.hist(X[n].flatten(), 100)\n",
    "ax1.set_xlim(lim_range)\n",
    "ax2.hist(predD1E10[n].flatten(), 100)\n",
    "ax2.set_xlim(lim_range)\n",
    "ax3.hist((X[n] - predD1E10[n]).flatten(), 100)\n",
    "ax3.set_xlim(-1,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=5\n",
    "lim_range= (0, 1)\n",
    "fig, (ax1, ax2, ax3)= plt.subplots(1, 3)\n",
    "ax1.hist(T[n].flatten(), 100)\n",
    "ax1.set_xlim(lim_range)\n",
    "ax2.hist(predD1E10T[n].flatten(), 100)\n",
    "ax2.set_xlim(lim_range)\n",
    "ax3.hist((T[n] - predD1E10T[n]).flatten(), 100)\n",
    "ax3.set_xlim(-1,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1 - dice_coef(y_true, y_pred)\n",
    "\n",
    "def binary_dice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) + (1 - dice_coef(y_true, y_pred))\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    \"\"\"\n",
    "    Dice = (2*|X & Y|)/ (|X|+ |Y|)\n",
    "         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n",
    "    ref: https://arxiv.org/pdf/1606.04797v1.pdf\n",
    "    \"\"\"\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\n",
    "\n",
    "def dice_coef1(y_true, y_pred, smooth=1):\n",
    "    \"\"\"\n",
    "    Dice = (2*|X & Y|)/ (|X|+ |Y|)\n",
    "         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n",
    "    ref: https://arxiv.org/pdf/1606.04797v1.pdf\n",
    "    \"\"\"\n",
    "    intersection = np.sum(np.abs(y_true * y_pred), axis=-1)\n",
    "    return (2. * intersection + smooth) / (np.sum(np.square(y_true),-1) + np.sum(np.square(y_pred),-1) + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_block(m, dim, acti, bn, res, do=0):\n",
    "    n = Conv2D(dim, 3, activation=acti, padding='same')(m)\n",
    "    n = BatchNormalization()(n) if bn else n\n",
    "    n = Dropout(do)(n) if do else n\n",
    "    n = Conv2D(dim, 3, activation=acti, padding='same')(n)\n",
    "    n = BatchNormalization()(n) if bn else n\n",
    "    return Concatenate()([m, n]) if res else n\n",
    "\n",
    "def level_block(m, dim, depth, inc, acti, do, bn, mp, up, res):\n",
    "    if depth > 0:\n",
    "        n = conv_block(m, dim, acti, bn, res)\n",
    "        m = MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)\n",
    "        m = level_block(m, int(inc*dim), depth-1, inc, acti, do, bn, mp, up, res)\n",
    "        if up:\n",
    "            m = UpSampling2D()(m)\n",
    "            m = Conv2D(dim, 2, activation=acti, padding='same')(m)\n",
    "        else:\n",
    "            m = Conv2DTranspose(dim, 3, strides=2, activation=acti, padding='same')(m)\n",
    "        n = Concatenate()([n, m])\n",
    "        m = conv_block(n, dim, acti, bn, res)\n",
    "    else:\n",
    "        m = conv_block(m, dim, acti, bn, res, do)\n",
    "    return m\n",
    "\n",
    "def UNet(img_shape, out_ch=1, start_ch=64, depth=4, inc_rate=2., activation='relu', \n",
    "         dropout=0.5, batchnorm=False, maxpool=True, upconv=True, residual=False):\n",
    "    i = Input(shape=img_shape)\n",
    "    o = level_block(i, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual)\n",
    "    o = Conv2D(out_ch, 1)(o)\n",
    "    o = Activation('sigmoid')(o)\n",
    "    return Model(inputs=i, outputs=o)\n",
    "\n",
    "def Logit_UNet(img_shape, out_ch=1, start_ch=64, depth=4, inc_rate=2., activation='relu', \n",
    "         dropout=0.5, batchnorm=False, maxpool=True, upconv=True, residual=False):\n",
    "    i = Input(shape=img_shape)\n",
    "    o = level_block(i, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual)\n",
    "    o = Conv2D(out_ch, 1)(o)\n",
    "    return Model(inputs=i, outputs=o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D1E10_comp1= np.load(mid_path + 'predD1E10.npy'); D1E10T_comp1= np.load(mid_path + 'predD1E10T.npy')\n",
    "D2E50_comp1= np.load(mid_path + 'predD2E50.npy'); D2E50T_comp1= np.load(mid_path + 'predD2E50T.npy')\n",
    "X= np.load(mid_path + 'X96.npy'); T= np.load(mid_path + 'T96.npy')\n",
    "y= np.load(mid_path + 'y96.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yin= np.expand_dims(y, axis= -1)\n",
    "yin.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train for predD2E50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x= D2E50_comp1.reshape(-1, 96 * 96); \n",
    "X1= (x- np.expand_dims(x.mean(axis=1), axis=1))/np.expand_dims(x.std(axis=1), axis=1); \n",
    "\n",
    "x= (X - D2E50_comp1).reshape(-1, 96 * 96); \n",
    "X2= (x- np.expand_dims(x.mean(axis=1), axis=1))/np.expand_dims(x.std(axis=1), axis=1); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xin= X1\n",
    "Xin= Xin.reshape(-1, 96, 96)\n",
    "Xin= np.expand_dims(Xin, axis= -1)\n",
    "Xin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shape= Xin[0].shape\n",
    "model = UNet((96, 96, 1), start_ch=16,depth=4,batchnorm=True)\n",
    "model.compile(optimizer = 'adadelta', loss = dice_coef_loss, metrics=[dice_coef])\n",
    "\n",
    "model_name= 'D2E50_comp1_seg'\n",
    "checkpoint1 = ModelCheckpoint(models_path + model_name + '_ep{epoch:02d}_best.hdf5', monitor='loss', verbose=1, save_weights_only=False, save_best_only=True, mode='min')\n",
    "checkpoint2 = ModelCheckpoint(models_path + model_name + '_weights_ep{epoch:02d}_best.hdf5', monitor='loss', verbose=1, save_weights_only=True, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint1, checkpoint2]\n",
    "\n",
    "epochs= 5\n",
    "batch_size= 128\n",
    "history= model.fit(Xin, yin, epochs=epochs,\n",
    "                   batch_size=batch_size, verbose=1,\n",
    "                   #validation_data=(X, y),\n",
    "                   callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xin= X2\n",
    "Xin= Xin.reshape(-1, 96, 96)\n",
    "Xin= np.expand_dims(Xin, axis= -1)\n",
    "Xin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shape= Xin[0].shape\n",
    "model = UNet((96, 96, 1), start_ch=16,depth=4,batchnorm=True)\n",
    "model.compile(optimizer = 'adadelta', loss = dice_coef_loss, metrics=[dice_coef])\n",
    "\n",
    "model_name= 'D2E50_comp2_seg'\n",
    "checkpoint1 = ModelCheckpoint(models_path + model_name + '_ep{epoch:02d}_best.hdf5', monitor='loss', verbose=1, save_weights_only=False, save_best_only=True, mode='min')\n",
    "checkpoint2 = ModelCheckpoint(models_path + model_name + '_weights_ep{epoch:02d}_best.hdf5', monitor='loss', verbose=1, save_weights_only=True, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint1, checkpoint2]\n",
    "\n",
    "epochs= 5\n",
    "batch_size= 128\n",
    "history= model.fit(Xin, yin, epochs=epochs,\n",
    "                   batch_size=batch_size, verbose=1,\n",
    "                   #validation_data=(X, y),\n",
    "                   callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train for predD1E10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x= (np.log1p(D1E10_comp1)).reshape(-1, 96 * 96); #t= D1E10T_comp1.reshape(-1, 96 * 96)\n",
    "X1= (x- np.expand_dims(x.mean(axis=1), axis=1))/np.expand_dims(x.std(axis=1), axis=1); \n",
    "\n",
    "x= (np.log1p((X - D1E10_comp1))).reshape(-1, 96 * 96); \n",
    "X2= (x- np.expand_dims(x.mean(axis=1), axis=1))/np.expand_dims(x.std(axis=1), axis=1); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xin= X1\n",
    "Xin= Xin.reshape(-1, 96, 96)\n",
    "Xin= np.expand_dims(Xin, axis= -1)\n",
    "Xin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shape= Xin[0].shape\n",
    "model = UNet((96, 96, 1), start_ch=16,depth=4,batchnorm=True)\n",
    "model.compile(optimizer = 'adadelta', loss = dice_coef_loss, metrics=[dice_coef])\n",
    "\n",
    "model_name= 'D1E10_comp1_seg'\n",
    "checkpoint1 = ModelCheckpoint(models_path + model_name + '_ep{epoch:02d}_best.hdf5', monitor='loss', verbose=1, save_weights_only=False, save_best_only=True, mode='min')\n",
    "checkpoint2 = ModelCheckpoint(models_path + model_name + '_weights_ep{epoch:02d}_best.hdf5', monitor='loss', verbose=1, save_weights_only=True, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint1, checkpoint2]\n",
    "\n",
    "epochs= 5\n",
    "batch_size= 128\n",
    "history= model.fit(Xin, yin, epochs=epochs,\n",
    "                   batch_size=batch_size, verbose=1,\n",
    "                   #validation_data=(X, y),\n",
    "                   callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xin= X2\n",
    "Xin= Xin.reshape(-1, 96, 96)\n",
    "Xin= np.expand_dims(Xin, axis= -1)\n",
    "Xin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shape= Xin[0].shape\n",
    "model = UNet((96, 96, 1), start_ch=16,depth=4,batchnorm=True)\n",
    "model.compile(optimizer = 'adadelta', loss = dice_coef_loss, metrics=[dice_coef])\n",
    "\n",
    "model_name= 'D1E10_comp2_seg'\n",
    "checkpoint1 = ModelCheckpoint(models_path + model_name + '_ep{epoch:02d}_best.hdf5', monitor='loss', verbose=1, save_weights_only=False, save_best_only=True, mode='min')\n",
    "checkpoint2 = ModelCheckpoint(models_path + model_name + '_weights_ep{epoch:02d}_best.hdf5', monitor='loss', verbose=1, save_weights_only=True, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint1, checkpoint2]\n",
    "\n",
    "epochs= 5\n",
    "batch_size= 128\n",
    "history= model.fit(Xin, yin, epochs=epochs,\n",
    "                   batch_size=batch_size, verbose=1,\n",
    "                   #validation_data=(X, y),\n",
    "                   callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D1E10_comp1= np.load(mid_path + 'predD1E10.npy'); D1E10T_comp1= np.load(mid_path + 'predD1E10T.npy')\n",
    "X= np.load(mid_path + 'X96.npy'); T= np.load(mid_path + 'T96.npy')\n",
    "y= np.load(mid_path + 'y96.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yin= np.expand_dims(y, axis= -1)\n",
    "yin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= D1E10_comp1.reshape(-1, 96 * 96); t= D1E10T_comp1.reshape(-1, 96 * 96)\n",
    "X1= (x- np.expand_dims(x.mean(axis=1), axis=1))/np.expand_dims(x.std(axis=1), axis=1); \n",
    "T1= (t- np.expand_dims(t.mean(axis=1), axis=1))/np.expand_dims(t.std(axis=1), axis=1)\n",
    "\n",
    "\n",
    "x= (X - D1E10_comp1).reshape(-1, 96 * 96); t= (T - D1E10T_comp1).reshape(-1, 96 * 96)\n",
    "X2= (x- np.expand_dims(x.mean(axis=1), axis=1))/np.expand_dims(x.std(axis=1), axis=1); \n",
    "T2= (t- np.expand_dims(t.mean(axis=1), axis=1))/np.expand_dims(t.std(axis=1), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "X1= X1.reshape(-1, 96, 96)\n",
    "X1= np.expand_dims(X1, axis= -1)\n",
    "print(X1.shape)\n",
    "\n",
    "X2= X2.reshape(-1, 96, 96)\n",
    "X2= np.expand_dims(X2, axis= -1)\n",
    "print(X2.shape)\n",
    "\n",
    "\n",
    "T1= T1.reshape(-1, 96, 96)\n",
    "T1= np.expand_dims(T1, axis= -1)\n",
    "print(T1.shape)\n",
    "\n",
    "\n",
    "T2= T2.reshape(-1, 96, 96)\n",
    "T2= np.expand_dims(T2, axis= -1)\n",
    "print(T2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = UNet((96, 96, 1), start_ch=16,depth=4,batchnorm=True)\n",
    "model.compile(optimizer = 'adadelta', loss = dice_coef_loss, metrics=[dice_coef])\n",
    "model.load_weights(models_path + 'D1E10_comp2_seg_weights_ep05_best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pX= model.predict(X2[:300])\n",
    "pT= model.predict(T2[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pX= np.squeeze(pX, axis=-1)\n",
    "pT= np.squeeze(pT, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pX.max(), pT.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level= pX.flatten()\n",
    "y_f= y[:300].flatten()\n",
    "dice= []\n",
    "thresholds= np.arange(0.1, level.max(), 0.01)\n",
    "for threshold in thresholds:\n",
    "    p_test= np.zeros((level.shape))\n",
    "\n",
    "    p_test[level > threshold]= 1\n",
    "    p_test= p_test.astype(np.float32)\n",
    "    y_true= y_f.astype(np.float32)\n",
    "\n",
    "    dice.append(dice_coef1(y_true, p_test.squeeze()))\n",
    "plt.plot(thresholds, dice)\n",
    "max_dice= np.array(dice).max()\n",
    "best_th= thresholds[np.argmax(np.array(dice))]\n",
    "print('max dice is ' + str(max_dice) + 'at threshold ' + str(best_th))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_finalX= np.zeros((pX.shape))\n",
    "p_finalX[pX > best_th]= 1\n",
    "\n",
    "p_finalT= np.zeros((pT.shape))\n",
    "p_finalT[pT > best_th]= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(0, 300, 5):\n",
    "    print(i)\n",
    "    show_im3(X[i], p_finalX[i]  , y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(0, 300, 5):\n",
    "    print(i)\n",
    "    show_im3(T[i], p_finalT[i]  , p_finalT[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train for X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x= X.reshape(-1, 96 * 96)\n",
    "Xin= (x- np.expand_dims(x.mean(axis=1), axis=1))/np.expand_dims(x.std(axis=1), axis=1); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xin= Xin.reshape(-1, 96, 96)\n",
    "Xin= np.expand_dims(Xin, axis= -1)\n",
    "Xin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shape= Xin[0].shape\n",
    "model = UNet((96, 96, 1), start_ch=16,depth=4,batchnorm=True)\n",
    "model.compile(optimizer = 'adadelta', loss = dice_coef_loss, metrics=[dice_coef])\n",
    "\n",
    "model_name= 'X_seg'\n",
    "checkpoint1 = ModelCheckpoint(models_path + model_name + '_ep{epoch:02d}_best.hdf5', monitor='loss', verbose=1, save_weights_only=False, save_best_only=True, mode='min')\n",
    "checkpoint2 = ModelCheckpoint(models_path + model_name + '_weights_ep{epoch:02d}_best.hdf5', monitor='loss', verbose=1, save_weights_only=True, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint1, checkpoint2]\n",
    "\n",
    "epochs= 5\n",
    "batch_size= 128\n",
    "history= model.fit(Xin, yin, epochs=epochs,\n",
    "                   batch_size=batch_size, verbose=1,\n",
    "                   #validation_data=(X, y),\n",
    "                   callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train for Log(1 + X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x= (np.log1p(X)).reshape(-1, 96 * 96)\n",
    "Xin= (x- np.expand_dims(x.mean(axis=1), axis=1))/np.expand_dims(x.std(axis=1), axis=1); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xin= Xin.reshape(-1, 96, 96)\n",
    "Xin= np.expand_dims(Xin, axis= -1)\n",
    "Xin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shape= Xin[0].shape\n",
    "model = UNet((96, 96, 1), start_ch=16,depth=4,batchnorm=True)\n",
    "model.compile(optimizer = 'adadelta', loss = dice_coef_loss, metrics=[dice_coef])\n",
    "\n",
    "model_name= 'log1pX_seg'\n",
    "checkpoint1 = ModelCheckpoint(models_path + model_name + '_ep{epoch:02d}_best.hdf5', monitor='loss', verbose=1, save_weights_only=False, save_best_only=True, mode='min')\n",
    "checkpoint2 = ModelCheckpoint(models_path + model_name + '_weights_ep{epoch:02d}_best.hdf5', monitor='loss', verbose=1, save_weights_only=True, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint1, checkpoint2]\n",
    "\n",
    "epochs= 50\n",
    "batch_size= 128\n",
    "history= model.fit(Xin, yin, epochs=epochs,\n",
    "                   batch_size=batch_size, verbose=1,\n",
    "                   #validation_data=(X, y),\n",
    "                   callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t= (np.log1p(T)).reshape(-1, 96 * 96)\n",
    "Tin= (t- np.expand_dims(t.mean(axis=1), axis=1))/np.expand_dims(t.std(axis=1), axis=1); \n",
    "Tin= Tin.reshape(-1, 96, 96)\n",
    "Tin= np.expand_dims(Tin, axis= -1)\n",
    "Tin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pX= model.predict(Xin[:300])\n",
    "pT= model.predict(Tin[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pX= np.squeeze(pX, axis=-1)\n",
    "pT= np.squeeze(pT, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level= pX.flatten()\n",
    "y_f= y[:300].flatten()\n",
    "dice= []\n",
    "thresholds= np.arange(0.1, level.max(), 0.01)\n",
    "for threshold in thresholds:\n",
    "    p_test= np.zeros((level.shape))\n",
    "\n",
    "    p_test[level > threshold]= 1\n",
    "    p_test= p_test.astype(np.float32)\n",
    "    y_true= y_f.astype(np.float32)\n",
    "\n",
    "    dice.append(dice_coef1(y_true, p_test.squeeze()))\n",
    "plt.plot(thresholds, dice)\n",
    "max_dice= np.array(dice).max()\n",
    "best_th= thresholds[np.argmax(np.array(dice))]\n",
    "print('max dice is ' + str(max_dice) + 'at threshold ' + str(best_th))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_finalX= np.zeros((pX.shape))\n",
    "p_finalX[pX > best_th]= 1\n",
    "\n",
    "p_finalT= np.zeros((pT.shape))\n",
    "p_finalT[pT > best_th]= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(0, 300, 5):\n",
    "    print(i)\n",
    "    show_im3(X[i], p_finalX[i]  , y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(0, 300, 5):\n",
    "    print(i)\n",
    "    show_im3(T[i], p_finalT[i]  , p_finalT[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D1E10_comp1= np.load(mid_path + 'predD1E10.npy'); D1E10T_comp1= np.load(mid_path + 'predD1E10T.npy')\n",
    "D2E50_comp1= np.load(mid_path + 'predD2E50.npy'); D2E50T_comp1= np.load(mid_path + 'predD2E50T.npy')\n",
    "X= np.load(mid_path + 'X96.npy'); T= np.load(mid_path + 'T96.npy')\n",
    "y= np.load(mid_path + 'y96.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x= X.reshape(-1, 96 * 96); t= T.reshape(-1, 96 * 96)\n",
    "orgX= (x- np.expand_dims(x.mean(axis=1), axis=1))/np.expand_dims(x.std(axis=1), axis=1); \n",
    "orgT= (t- np.expand_dims(t.mean(axis=1), axis=1))/np.expand_dims(t.std(axis=1), axis=1); \n",
    "\n",
    "x= (np.log1p(X)).reshape(-1, 96 * 96); t= (np.log1p(T)).reshape(-1, 96 * 96)\n",
    "log1pX= (x- np.expand_dims(x.mean(axis=1), axis=1))/np.expand_dims(x.std(axis=1), axis=1); \n",
    "log1pT= (t- np.expand_dims(t.mean(axis=1), axis=1))/np.expand_dims(t.std(axis=1), axis=1); \n",
    "\n",
    "x= D1E10_comp1.reshape(-1, 96 * 96); t= D1E10T_comp1.reshape(-1, 96 * 96)\n",
    "D1E10_comp1X= (x- np.expand_dims(x.mean(axis=1), axis=1))/np.expand_dims(x.std(axis=1), axis=1); \n",
    "D1E10_comp1T= (t- np.expand_dims(t.mean(axis=1), axis=1))/np.expand_dims(t.std(axis=1), axis=1)\n",
    "\n",
    "\n",
    "x= (X - D1E10_comp1).reshape(-1, 96 * 96); t= (T - D1E10T_comp1).reshape(-1, 96 * 96)\n",
    "D1E10_comp2X= (x- np.expand_dims(x.mean(axis=1), axis=1))/np.expand_dims(x.std(axis=1), axis=1); \n",
    "D1E10_comp2T= (t- np.expand_dims(t.mean(axis=1), axis=1))/np.expand_dims(t.std(axis=1), axis=1)\n",
    "\n",
    "x= D2E50_comp1.reshape(-1, 96 * 96); \n",
    "D2E50_comp1X= (x- np.expand_dims(x.mean(axis=1), axis=1))/np.expand_dims(x.std(axis=1), axis=1); \n",
    "D2E50_comp1T= (t- np.expand_dims(t.mean(axis=1), axis=1))/np.expand_dims(t.std(axis=1), axis=1)\n",
    "\n",
    "x= (X - D2E50_comp1).reshape(-1, 96 * 96); \n",
    "D2E50_comp2X= (x- np.expand_dims(x.mean(axis=1), axis=1))/np.expand_dims(x.std(axis=1), axis=1); \n",
    "D2E50_comp2T= (t- np.expand_dims(t.mean(axis=1), axis=1))/np.expand_dims(t.std(axis=1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level1X= []\n",
    "level1T= []\n",
    "a= ['X_seg', 'log1pX_seg', 'D1E10_comp1_seg', 'D1E10_comp2_seg', 'D2E50_comp1_seg', 'D2E50_comp2_seg'] \n",
    "b= [orgX, log1pX, D1E10_comp1X, D1E10_comp2X, D2E50_comp1X, D2E50_comp2X]\n",
    "c= [orgT, log1pT, D1E10_comp1T, D1E10_comp2T, D2E50_comp1T, D2E50_comp2T]\n",
    "\n",
    "for m, data, dataT in zip(a, b, c):\n",
    "    print(m)\n",
    "    data= data.reshape(-1, 96, 96); data= np.expand_dims(data, axis= -1)\n",
    "    dataT= dataT.reshape(-1, 96, 96); dataT= np.expand_dims(dataT, axis= -1)\n",
    "    \n",
    "    model = UNet((96, 96, 1), start_ch=16,depth=4,batchnorm=True)\n",
    "    model.compile(optimizer = 'adadelta', loss = dice_coef_loss, metrics=[dice_coef])\n",
    "    model.load_weights(models_path + m + '_weights_ep05_best.hdf5')\n",
    "    #level1X.append(model.predict(data).flatten())\n",
    "    level1T.append(model.predict(dataT).flatten())\n",
    "    \n",
    "#level1X= np.array(level1X).T \n",
    "level1T= np.array(level1T).T\n",
    "#np.save(mid_path + 'level1X', level1X); \n",
    "np.save(mid_path + 'level1T', level1T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "level1= np.load(mid_path + 'level1X.npy'); level1T= np.load(mid_path + 'level1T.npy')\n",
    "#y_f= np.load(mid_path + 'y96.npy').flatten();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best1= []\n",
    "level= level1\n",
    "for col in range(level.shape[1]):\n",
    "    dice= []\n",
    "    thresholds= np.arange(0.1, level[:, col].max(), 0.01)\n",
    "    for threshold in thresholds:\n",
    "        p_test= np.zeros((level.shape[0], 1))\n",
    "\n",
    "        p_test[level[:, col] > threshold]= 1\n",
    "        p_test= p_test.astype(np.float32)\n",
    "        y_true= y_f.astype(np.float32)\n",
    "\n",
    "        dice.append(dice_coef1(y_true, p_test.squeeze()))\n",
    "    plt.plot(thresholds, dice)\n",
    "    max_dice= np.array(dice).max()\n",
    "    best_th= thresholds[np.argmax(np.array(dice))]\n",
    "    best1.append(best_th)\n",
    "    print('max dice is ' + str(max_dice) + ' at threshold ' + str(best_th))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_finalT= np.zeros((level1T.shape[0], 1))\n",
    "p_finalT[level1T[:, 3] > 0.15]= 1\n",
    "\n",
    "# X 0.39\n",
    "# log 0.49\n",
    "#     0.5\n",
    "# 0.31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "level1X= np.load(mid_path + 'level1X.npy'); \n",
    "level1T= np.load(mid_path + 'level1T.npy'); \n",
    "y_f= np.load(mid_path + 'y96.npy').flatten(); \n",
    "X96= np.load(mid_path + 'X96.npy')\n",
    "T96= np.load(mid_path + 'T96.npy')\n",
    "d_f= np.load(mid_path + 'd_f96.npy'); dt_f= np.load(mid_path + 'dt_f96.npy'); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_f= d_f.reshape(-1, 1)\n",
    "dt_f= dt_f.reshape(-1, 1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "limit= d_f.shape[0]\n",
    "d_N= scaler.fit_transform(np.vstack((d_f, dt_f))).astype(np.float32)\n",
    "d_f= d_N[:limit, :]\n",
    "dt_f= d_N[limit:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_f.shape, level1T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_level2= level1X\n",
    "T_level2= level1T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_level2= np.hstack((X_level2, d_f)) \n",
    "T_level2= np.hstack((T_level2, dt_f)) \n",
    "X_level2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level2= []\n",
    "level2T= []\n",
    "\n",
    "print('BR')\n",
    "level2_BRmodel= BayesianRidge()\n",
    "level2_BRmodel.fit(X_level2, y_f)\n",
    "level2_pBR= level2_BRmodel.predict(X_level2)\n",
    "\n",
    "\n",
    "print(level2_pBR.max(), level2_BRmodel.coef_)\n",
    "level2.append(level2_pBR)\n",
    "\n",
    "level2T.append(level2_BRmodel.predict(T_level2))\n",
    "\n",
    "\n",
    "print('LGB')\n",
    "lgb_params= {'feature_fraction': 0.7,'metric': 'rmse', 'nthread':12, 'min_data_in_leaf': 2**7, \n",
    "              'bagging_fraction': 0.7, 'learning_rate': 0.05, 'objective': 'rmse',\n",
    "              'bagging_seed': 2**5, 'num_leaves': 2**11,'bagging_freq':1,'verbosity': 50}\n",
    "\n",
    "level2_LGBmodel = lgb.train(lgb_params, lgb.Dataset(X_level2, label=y_f), 50)\n",
    "p= level2_LGBmodel.predict(X_level2)\n",
    "weight= y_f-p + 1\n",
    "\n",
    "print('LGB_error')\n",
    "level2_LGBmodel1 = lgb.train(lgb_params, lgb.Dataset(X_level2, label=y_f, weight=weight), 300)\n",
    "p1= level2_LGBmodel1.predict(X_level2)\n",
    "error1= y_f-p1\n",
    "\n",
    "pT1= level2_LGBmodel1.predict(T_level2)\n",
    "\n",
    "print('LGB_error1')\n",
    "level2_LGBmodel2 = lgb.train(lgb_params, lgb.Dataset(X_level2, label=error1), 300)\n",
    "p2= level2_LGBmodel2.predict(X_level2)\n",
    "\n",
    "pT2= level2_LGBmodel2.predict(T_level2)\n",
    "\n",
    "level2.append(p1 + p2)\n",
    "level2T.append(pT1 + pT2)\n",
    "#ax = lgb.plot_importance(level2_LGBmodel, figsize=(12, 30));plt.show()\n",
    "\n",
    "level2= np.array(level2).T \n",
    "level2T= np.array(level2T).T \n",
    "\n",
    "level2.max(axis=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = lgb.plot_importance(level2_LGBmodel, figsize=(12, 30));plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "level2_LGBmodel1.save_model('level2_LGBmodel1.txt')\n",
    "level2_LGBmodel2.save_model('level2_LGBmodel2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(mid_path + 'level2X', level2); np.save(mid_path + 'level2T', level2T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "level2= np.load(mid_path + 'level2X.npy'); level2T= np.load(mid_path + 'level2T.npy')\n",
    "y_f= np.load(mid_path + 'y96.npy').flatten();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level= level2\n",
    "for col in range(level.shape[1]):\n",
    "    dice= []\n",
    "    thresholds= np.arange(0.1, level[:, col].max(), 0.01)\n",
    "    for threshold in thresholds:\n",
    "        p_test= np.zeros((level.shape[0], 1))\n",
    "\n",
    "        p_test[level[:, col] > threshold]= 1\n",
    "        p_test= p_test.astype(np.float32)\n",
    "        y_true= y_f.astype(np.float32)\n",
    "\n",
    "        dice.append(dice_coef1(y_true, p_test.squeeze()))\n",
    "    plt.plot(thresholds, dice)\n",
    "    max_dice= np.array(dice).max()\n",
    "    best_th= thresholds[np.argmax(np.array(dice))]\n",
    "    print('max dice is ' + str(max_dice) + ' at threshold ' + str(best_th))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_final= np.zeros((level2.shape[0], 1))\n",
    "p_final[level2[:, 0] > best_th]= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_finalT= np.zeros((level2T.shape[0], 1))\n",
    "p_finalT[level2T[:, 0] > best_th]= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp= p_final.reshape((-1, 96, 96))\n",
    "y_c= y_f.reshape((-1, 96, 96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(0, 300, 5):\n",
    "    print(i)\n",
    "    show_im3(X96[i], y_c[i], imp[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "impT= p_finalT[:96 * 96 * 300].reshape((-1, 96, 96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(0, 300, 2):\n",
    "    print(i)\n",
    "    show_im2(T96[i], impT[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names= np.load(mid_path + 'names.npy'); names0= np.load(mid_path + 'names0.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_final= p_finalT.reshape((-1, 96, 96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rle_dict= {}\n",
    "for im, name in zip(pred_final, names):\n",
    "    im= np.squeeze(im)\n",
    "    im= cv2.resize(im, (101, 101))\n",
    "    rle_dict[name]= rle(im)\n",
    "\n",
    "rle_dict0= {}\n",
    "for name0 in names0:\n",
    "    rle_dict0[name0]= rle(np.zeros((101, 101)))\n",
    "\n",
    "rle_dict.update(rle_dict0)\n",
    "\n",
    "sub = pd.DataFrame.from_dict(rle_dict, orient='index')\n",
    "sub.index.names = ['id']\n",
    "sub.columns = ['rle_mask']\n",
    "sub.to_csv('True_submissionCNN_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
